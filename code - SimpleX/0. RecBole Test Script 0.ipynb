{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2544fba4-f427-4878-a8c9-dde77d248c5d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ABOUT\n",
    "- this notebook explores the use of RecBole API - a python package for recommendation algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e8a14a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Run 0 - lightgcn\n",
    "- run lightgcn with recbole \n",
    "    - appears to take quite some time for even 2 layers\n",
    "    - maybe we focus on 2 layers max for now due to resources\n",
    "    - get more compute resources from prof when the projecy matures?\n",
    "- tried out load_data_and_model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da17d73f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run_recbole contains all the necessary components in recommendation pipeline\n",
    "from recbole.quick_start import run_recbole\n",
    "run_recbole(model='LightGCN', dataset='ml-100k', config_file_list=['test0.yaml'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5c09f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# after pipeline completion we can load all info - load_data_and_model\n",
    "from recbole.quick_start import run_recbole, load_data_and_model\n",
    "path = r\"C:\\Users\\tanch\\Documents\\NTU\\NTU Year 4\\FYP - GNN\\code\\saved\\LightGCN-Aug-22-2022_14-17-31.pth\"\n",
    "config, model, dataset, train_data, valid_data, test_data = load_data_and_model(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a0a07a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a36cbf3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dir(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144a2b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666d6549",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.dataset, test_data.dataset, valid_data.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915a64eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(train_data.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec66c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.dataset.dataset_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e80d889",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# run 1 - gowalla\n",
    "- downloaded gowalla dataset\n",
    "- used recbole api to load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac91b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from logging import getLogger\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "from recbole.config import Config\n",
    "from recbole.data import (\n",
    "    create_dataset,\n",
    "    data_preparation,\n",
    "    save_split_dataloaders,\n",
    "    load_split_dataloaders,\n",
    ")\n",
    "from recbole.utils import (\n",
    "    init_logger,\n",
    "    get_model,\n",
    "    get_trainer,\n",
    "    init_seed,\n",
    "    set_color,\n",
    "#     get_flops,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cca0f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"Pop\"\n",
    "dataset = \"gowalla\"\n",
    "config_file_list = [\"test0.yaml\"] \n",
    "config_dict = None\n",
    "config = Config(\n",
    "    model=model,\n",
    "    dataset=dataset,\n",
    "    config_file_list=config_file_list,\n",
    "    config_dict=config_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aeb3489",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_seed(config[\"seed\"], config[\"reproducibility\"]) \n",
    "config[\"seed\"], config[\"reproducibility\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be497c3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# logger initialization\n",
    "init_logger(config)\n",
    "logger = getLogger()\n",
    "logger.info(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f408d91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset filtering\n",
    "dataset = create_dataset(config)\n",
    "logger.info(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80563ae7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# run 2 - @k\n",
    "- to set @k for metrics use define \"topk\" args\n",
    "- cant use LogLoss somehow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8945406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from logging import getLogger\n",
    "\n",
    "import sys\n",
    "\n",
    "import pickle\n",
    "\n",
    "from recbole.config import Config\n",
    "from recbole.data import (\n",
    "    create_dataset,\n",
    "    data_preparation,\n",
    "    save_split_dataloaders,\n",
    "    load_split_dataloaders,\n",
    ")\n",
    "from recbole.utils import (\n",
    "    init_logger,\n",
    "    get_model,\n",
    "    get_trainer,\n",
    "    init_seed,\n",
    "    set_color,\n",
    "#     get_flops,\n",
    ")\n",
    "\n",
    "model = \"Pop\"\n",
    "dataset = \"ml-100k\"\n",
    "config_file_list = [\"test0.yaml\"] \n",
    "config_dict = None\n",
    "saved = False\n",
    "\n",
    "\n",
    "config = Config(\n",
    "    model=model,\n",
    "    dataset=dataset,\n",
    "    config_file_list=config_file_list,\n",
    "    config_dict=config_dict,\n",
    ")\n",
    "\n",
    "init_seed(config[\"seed\"], config[\"reproducibility\"]) \n",
    "config[\"seed\"], config[\"reproducibility\"]\n",
    "\n",
    "# logger initialization\n",
    "init_logger(config)\n",
    "logger = getLogger()\n",
    "logger.info(config)\n",
    "\n",
    "# dataset filtering\n",
    "dataset = create_dataset(config)\n",
    "logger.info(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e13169",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dataset splitting\n",
    "train_data, valid_data, test_data = data_preparation(config, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ad7d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model loading and initialization\n",
    "init_seed(config['seed'], config['reproducibility'])\n",
    "model = get_model(config['model'])(config, train_data.dataset).to(config['device'])\n",
    "logger.info(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e0eebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer loading and initialization\n",
    "trainer = get_trainer(config['MODEL_TYPE'], config['model'])(config, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46f1ad9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model training\n",
    "best_valid_score, best_valid_result = trainer.fit(\n",
    "    train_data, valid_data, saved=saved, show_progress=config['show_progress']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cc4560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model evaluation\n",
    "test_result = trainer.evaluate(test_data, load_best_model=saved, show_progress=config['show_progress'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21430ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logger.info(set_color('best valid ', 'yellow') + f': {best_valid_result}')\n",
    "logger.info(set_color('test result', 'yellow') + f': {test_result}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31447105",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "        'best_valid_score': best_valid_score,\n",
    "        'valid_score_bigger': config['valid_metric_bigger'],\n",
    "        'best_valid_result': best_valid_result,\n",
    "        'test_result': test_result\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a367ed0b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# run 3 - wandb\n",
    "- run with wandb turned on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f544fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from logging import getLogger\n",
    "\n",
    "import sys\n",
    "\n",
    "import pickle\n",
    "\n",
    "from recbole.config import Config\n",
    "from recbole.data import (\n",
    "    create_dataset,\n",
    "    data_preparation,\n",
    "    save_split_dataloaders,\n",
    "    load_split_dataloaders,\n",
    ")\n",
    "from recbole.utils import (\n",
    "    init_logger,\n",
    "    get_model,\n",
    "    get_trainer,\n",
    "    init_seed,\n",
    "    set_color,\n",
    "#     get_flops,\n",
    ")\n",
    "\n",
    "model = \"Pop\"\n",
    "dataset = \"ml-100k\"\n",
    "config_file_list = [\"test0.yaml\"] \n",
    "config_dict = None\n",
    "saved = False\n",
    "\n",
    "\n",
    "config = Config(\n",
    "    model=model,\n",
    "    dataset=dataset,\n",
    "    config_file_list=config_file_list,\n",
    "    config_dict=config_dict,\n",
    ")\n",
    "\n",
    "init_seed(config[\"seed\"], config[\"reproducibility\"]) \n",
    "config[\"seed\"], config[\"reproducibility\"]\n",
    "\n",
    "# logger initialization\n",
    "init_logger(config)\n",
    "logger = getLogger()\n",
    "logger.info(config)\n",
    "\n",
    "# dataset filtering\n",
    "dataset = create_dataset(config)\n",
    "logger.info(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e83650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset splitting\n",
    "train_data, valid_data, test_data = data_preparation(config, dataset)\n",
    "\n",
    "# model loading and initialization\n",
    "init_seed(config['seed'], config['reproducibility'])\n",
    "model = get_model(config['model'])(config, train_data.dataset).to(config['device'])\n",
    "logger.info(model)\n",
    "\n",
    "# trainer loading and initialization\n",
    "trainer = get_trainer(config['MODEL_TYPE'], config['model'])(config, model)\n",
    "\n",
    "# model training\n",
    "best_valid_score, best_valid_result = trainer.fit(\n",
    "    train_data, valid_data, saved=saved, show_progress=config['show_progress']\n",
    ")\n",
    "\n",
    "# model evaluation\n",
    "test_result = trainer.evaluate(test_data, load_best_model=saved, show_progress=config['show_progress'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984f460e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# run 4 - SimpleX\n",
    "- couldn't run SimpleX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f893899f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from logging import getLogger\n",
    "\n",
    "import sys\n",
    "\n",
    "import pickle\n",
    "\n",
    "from recbole.config import Config\n",
    "from recbole.data import (\n",
    "    create_dataset,\n",
    "    data_preparation,\n",
    "    save_split_dataloaders,\n",
    "    load_split_dataloaders,\n",
    ")\n",
    "from recbole.utils import (\n",
    "    init_logger,\n",
    "    get_model,\n",
    "    get_trainer,\n",
    "    init_seed,\n",
    "    set_color,\n",
    "#     get_flops,\n",
    ")\n",
    "\n",
    "model = \"SimpleX\"\n",
    "dataset = \"ml-100k\"\n",
    "config_file_list = [\"test0.yaml\"] \n",
    "config_dict = {\n",
    "    \"gamma\":0.5,\n",
    "    \"aggregator\": \"mean\",\n",
    "    \"reg_weight\":0,\n",
    "    \"margin\":0,\n",
    "    \"negative_weight\":300,\n",
    "    \n",
    "}\n",
    "saved = False\n",
    "\n",
    "\n",
    "config = Config(\n",
    "    model=model,\n",
    "    dataset=dataset,\n",
    "    config_file_list=config_file_list,\n",
    "    config_dict=config_dict,\n",
    ")\n",
    "\n",
    "init_seed(config[\"seed\"], config[\"reproducibility\"]) \n",
    "config[\"seed\"], config[\"reproducibility\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8077f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ed3231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger initialization\n",
    "init_logger(config)\n",
    "logger = getLogger()\n",
    "logger.info(config)\n",
    "\n",
    "# dataset filtering\n",
    "dataset = create_dataset(config)\n",
    "logger.info(dataset)\n",
    "\n",
    "# dataset splitting\n",
    "train_data, valid_data, test_data = data_preparation(config, dataset)\n",
    "\n",
    "# model loading and initialization\n",
    "init_seed(config['seed'], config['reproducibility'])\n",
    "model = get_model(config['model'])(config, train_data.dataset).to(config['device'])\n",
    "logger.info(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc90a616",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# trainer loading and initialization\n",
    "trainer = get_trainer(config['MODEL_TYPE'], config['model'])(config, model)\n",
    "\n",
    "# model training\n",
    "best_valid_score, best_valid_result = trainer.fit(\n",
    "    train_data, valid_data, saved=saved, show_progress=config['show_progress']\n",
    ")\n",
    "\n",
    "# model evaluation\n",
    "# test_result = trainer.evaluate(test_data, load_best_model=saved, show_progress=config['show_progress'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9443b5b8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# run 5 - LightGCN\n",
    "- run using run_recbole API\n",
    "- likely better to write a customize function to run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cee4329",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"LightGCN\"\n",
    "dataset = 'ml-100k'\n",
    "config_file_list = ['test0.yaml']\n",
    "config_dict = {\n",
    "    \"show_progress\": False,\n",
    "    \"log_wandb\":True,\n",
    "    \"eval_step\": 10,\n",
    "    \"epochs\": 50, \n",
    "    \"RS\": [6,2,2],\n",
    "    \"valid_metric\":\"NDCG@20\",\n",
    "    \"metrics\":[\"Recall\",\"NDCG\"],\n",
    "    \"topk\":[10,20],\n",
    "    \"embedding_size\":32,\n",
    "    \"n_layers\":2,\n",
    "    \"reg_weight\":0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcb046f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_recbole contains all the necessary components in recommendation pipeline\n",
    "from recbole.quick_start import run_recbole\n",
    "run_recbole(model=model, dataset=dataset, \n",
    "            config_file_list=config_file_list,\n",
    "            config_dict = config_dict\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c353da4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# run 6 - SimpleX without runrecbole API\n",
    "- it appears that SimplX is very sensitive to \"margin\" hyperparameter\n",
    "- so far it doesn't seem to be anywhere close to LightGCN performance despite both having the same number of parameters\n",
    "- training of LightgCN appears to be faster\n",
    "- successful in resuming from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0572d127",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"SimpleX\"\n",
    "dataset = 'ml-100k'\n",
    "config_file_list = ['test0.yaml']\n",
    "config_dict = {\n",
    "    \"show_progress\": False,\n",
    "    \"log_wandb\":True,\n",
    "    \"eval_step\": 10,\n",
    "    \"epochs\": 50, \n",
    "    \"RS\": [6,2,2],\n",
    "    \"valid_metric\":\"NDCG@20\",\n",
    "    \"metrics\":[\"Recall\",\"NDCG\"],\n",
    "    \"topk\":[10,20],\n",
    "    \n",
    "    \"embedding_size\":32,\n",
    "    \"margin\":0.8,\n",
    "    \"negative_weight\":100,\n",
    "    \"gamma\":0.5,\n",
    "    \"reg_weight\":0,\n",
    "    \"aggregator\":\"mean\",\n",
    "    \"history_len\":1000\n",
    "}\n",
    "saved = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb04ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "recbole.quick_start\n",
    "########################\n",
    "\"\"\"\n",
    "import logging\n",
    "from logging import getLogger\n",
    "\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "from recbole.config import Config\n",
    "from recbole.data import create_dataset, data_preparation, save_split_dataloaders, load_split_dataloaders\n",
    "from recbole.utils import init_logger, get_model, get_trainer, init_seed, set_color\n",
    "\n",
    "\n",
    "# def run_recbole(model=None, dataset=None, config_file_list=None, config_dict=None, saved=True):\n",
    "\n",
    "# configurations initialization\n",
    "config = Config(model=model, dataset=dataset, config_file_list=config_file_list, config_dict=config_dict)\n",
    "init_seed(config['seed'], config['reproducibility'])\n",
    "# logger initialization\n",
    "init_logger(config)\n",
    "logger = getLogger()\n",
    "\n",
    "logger.info(config)\n",
    "\n",
    "# dataset filtering\n",
    "dataset = create_dataset(config)\n",
    "logger.info(dataset)\n",
    "\n",
    "# dataset splitting\n",
    "train_data, valid_data, test_data = data_preparation(config, dataset)\n",
    "\n",
    "# model loading and initialization\n",
    "init_seed(config['seed'], config['reproducibility'])\n",
    "model = get_model(config['model'])(config, train_data.dataset).to(config['device'])\n",
    "logger.info(model)\n",
    "\n",
    "# # trainer loading and initialization\n",
    "# trainer = get_trainer(config['MODEL_TYPE'], config['model'])(config, model)\n",
    "\n",
    "# # model training\n",
    "# best_valid_score, best_valid_result = trainer.fit(\n",
    "#     train_data, valid_data, saved=saved, show_progress=config['show_progress']\n",
    "# )\n",
    "\n",
    "# # model evaluation\n",
    "# test_result = trainer.evaluate(test_data, load_best_model=saved, show_progress=config['show_progress'])\n",
    "\n",
    "# logger.info(set_color('best valid ', 'yellow') + f': {best_valid_result}')\n",
    "# logger.info(set_color('test result', 'yellow') + f': {test_result}')\n",
    "\n",
    "# {\n",
    "#     'best_valid_score': best_valid_score,\n",
    "#     'valid_score_bigger': config['valid_metric_bigger'],\n",
    "#     'best_valid_result': best_valid_result,\n",
    "#     'test_result': test_result\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026d05c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer loading and initialization\n",
    "trainer = get_trainer(config['MODEL_TYPE'], config['model'])(config, model)\n",
    "\n",
    "# resume checkpoint\n",
    "resume_file = r\"C:\\Users\\tanch\\Documents\\NTU\\NTU Year 4\\FYP - GNN\\code\\saved\\SimpleX-Sep-01-2022_11-29-26.pth\"\n",
    "trainer.resume_checkpoint(resume_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d1cc1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model training\n",
    "best_valid_score, best_valid_result = trainer.fit(\n",
    "    train_data, valid_data, saved=saved, show_progress=config['show_progress']\n",
    ")\n",
    "\n",
    "# model evaluation\n",
    "test_result = trainer.evaluate(test_data, load_best_model=saved, show_progress=config['show_progress'])\n",
    "\n",
    "logger.info(set_color('best valid ', 'yellow') + f': {best_valid_result}')\n",
    "logger.info(set_color('test result', 'yellow') + f': {test_result}')\n",
    "\n",
    "{\n",
    "    'best_valid_score': best_valid_score,\n",
    "    'valid_score_bigger': config['valid_metric_bigger'],\n",
    "    'best_valid_result': best_valid_result,\n",
    "    'test_result': test_result\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9c0f51",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# run 7 - SimpleX \n",
    "- it appears that the negative samples were sampled in the dataloader, we we gotta check the dataloader script to control the number of negative samples, which is an important hyperparamter for simplex\n",
    "- so in a single batch there is 1 pos samples and n negative samples, what is the parameter name for this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86b64f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"SimpleX\"\n",
    "dataset = 'ml-100k'\n",
    "config_file_list = ['test0.yaml']\n",
    "config_dict = {\n",
    "    \"show_progress\": False,\n",
    "    \"log_wandb\":True,\n",
    "    \"eval_step\": 10,\n",
    "    \"epochs\": 50, \n",
    "    \"eval_args\": {\"split\": {\"RS\": [6,2,2]}},\n",
    "    \"valid_metric\":\"NDCG@20\",\n",
    "    \"metrics\":[\"Recall\",\"NDCG\"],\n",
    "    \"topk\":[10,20],\n",
    "    \n",
    "    \"embedding_size\":32,\n",
    "    \"margin\":0.8,\n",
    "    \"negative_weight\":100,\n",
    "    \"gamma\":0.5,\n",
    "    \"reg_weight\":0,\n",
    "    \"aggregator\":\"mean\",\n",
    "    \"history_len\":10,\n",
    "}\n",
    "saved = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0937b8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tanch\\anaconda3\\envs\\fyp0\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "01 Sep 12:35    INFO  \n",
      "General Hyper Parameters:\n",
      "gpu_id = 0\n",
      "use_gpu = True\n",
      "seed = 2020\n",
      "state = INFO\n",
      "reproducibility = True\n",
      "data_path = C:\\Users\\tanch\\anaconda3\\envs\\fyp0\\Lib\\site-packages\\recbole\\config\\../dataset_example/ml-100k\n",
      "checkpoint_dir = saved/\n",
      "show_progress = False\n",
      "save_dataset = False\n",
      "dataset_save_path = None\n",
      "save_dataloaders = False\n",
      "dataloaders_save_path = None\n",
      "log_wandb = True\n",
      "\n",
      "Training Hyper Parameters:\n",
      "epochs = 50\n",
      "train_batch_size = 2048\n",
      "learner = adam\n",
      "learning_rate = 0.001\n",
      "neg_sampling = {'uniform': 1}\n",
      "eval_step = 10\n",
      "stopping_step = 10\n",
      "clip_grad_norm = None\n",
      "weight_decay = 0.0\n",
      "loss_decimal_place = 4\n",
      "\n",
      "Evaluation Hyper Parameters:\n",
      "eval_args = {'split': {'RS': [6, 2, 2]}, 'order': 'RO', 'group_by': 'user', 'mode': 'full'}\n",
      "repeatable = False\n",
      "metrics = ['Recall', 'NDCG']\n",
      "topk = [10, 20]\n",
      "valid_metric = NDCG@20\n",
      "valid_metric_bigger = True\n",
      "eval_batch_size = 4096\n",
      "metric_decimal_place = 4\n",
      "\n",
      "Dataset Hyper Parameters:\n",
      "field_separator = \t\n",
      "seq_separator =  \n",
      "USER_ID_FIELD = user_id\n",
      "ITEM_ID_FIELD = item_id\n",
      "RATING_FIELD = rating\n",
      "TIME_FIELD = timestamp\n",
      "seq_len = None\n",
      "LABEL_FIELD = label\n",
      "threshold = None\n",
      "NEG_PREFIX = neg_\n",
      "load_col = {'inter': ['user_id', 'item_id']}\n",
      "unload_col = None\n",
      "unused_col = None\n",
      "additional_feat_suffix = None\n",
      "rm_dup_inter = None\n",
      "val_interval = None\n",
      "filter_inter_by_user_or_item = True\n",
      "user_inter_num_interval = None\n",
      "item_inter_num_interval = None\n",
      "alias_of_user_id = None\n",
      "alias_of_item_id = None\n",
      "alias_of_entity_id = None\n",
      "alias_of_relation_id = None\n",
      "preload_weight = None\n",
      "normalize_field = None\n",
      "normalize_all = None\n",
      "ITEM_LIST_LENGTH_FIELD = item_length\n",
      "LIST_SUFFIX = _list\n",
      "MAX_ITEM_LIST_LENGTH = 50\n",
      "POSITION_FIELD = position_id\n",
      "HEAD_ENTITY_ID_FIELD = head_id\n",
      "TAIL_ENTITY_ID_FIELD = tail_id\n",
      "RELATION_ID_FIELD = relation_id\n",
      "ENTITY_ID_FIELD = entity_id\n",
      "benchmark_filename = None\n",
      "\n",
      "Other Hyper Parameters: \n",
      "wandb_project = recbole\n",
      "require_pow = False\n",
      "MODEL_TYPE = ModelType.GENERAL\n",
      "worker = 0\n",
      "shuffle = True\n",
      "embedding_size = 32\n",
      "train_neg_sample_args = {'strategy': 'by', 'by': 1, 'distribution': 'uniform', 'dynamic': 'none'}\n",
      "margin = 0.8\n",
      "negative_weight = 100\n",
      "gamma = 0.5\n",
      "reg_weight = 0\n",
      "aggregator = mean\n",
      "history_len = 10\n",
      "MODEL_INPUT_TYPE = InputType.PAIRWISE\n",
      "eval_type = EvaluatorType.RANKING\n",
      "device = cuda\n",
      "eval_neg_sample_args = {'strategy': 'full', 'distribution': 'uniform'}\n",
      "\n",
      "\n",
      "01 Sep 12:35    INFO  ml-100k\n",
      "The number of users: 944\n",
      "Average actions of users: 106.04453870625663\n",
      "The number of items: 1683\n",
      "Average actions of items: 59.45303210463734\n",
      "The number of inters: 100000\n",
      "The sparsity of the dataset: 93.70575143257098%\n",
      "Remain Fields: ['user_id', 'item_id']\n",
      "01 Sep 12:35    INFO  [Training]: train_batch_size = [2048] negative sampling: [{'uniform': 1}]\n",
      "01 Sep 12:35    INFO  [Evaluation]: eval_batch_size = [4096] eval_args: [{'split': {'RS': [6, 2, 2]}, 'order': 'RO', 'group_by': 'user', 'mode': 'full'}]\n",
      "01 Sep 12:35    WARNING  Max value of user's history interaction records has reached 26.322043969102793% of the total.\n",
      "01 Sep 12:35    INFO  SimpleX(\n",
      "  (user_emb): Embedding(944, 32)\n",
      "  (item_emb): Embedding(1683, 32, padding_idx=0)\n",
      "  (UI_map): Linear(in_features=32, out_features=32, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (reg_loss): EmbLoss()\n",
      ")\n",
      "Trainable parameters: 85088\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "recbole.quick_start\n",
    "########################\n",
    "\"\"\"\n",
    "import logging\n",
    "from logging import getLogger\n",
    "\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "from recbole.config import Config\n",
    "from recbole.data import create_dataset, data_preparation, save_split_dataloaders, load_split_dataloaders\n",
    "from recbole.utils import init_logger, get_model, get_trainer, init_seed, set_color\n",
    "\n",
    "\n",
    "# def run_recbole(model=None, dataset=None, config_file_list=None, config_dict=None, saved=True):\n",
    "\n",
    "# configurations initialization\n",
    "config = Config(model=model, dataset=dataset, config_file_list=config_file_list, config_dict=config_dict)\n",
    "init_seed(config['seed'], config['reproducibility'])\n",
    "# logger initialization\n",
    "init_logger(config)\n",
    "logger = getLogger()\n",
    "\n",
    "logger.info(config)\n",
    "\n",
    "# dataset filtering\n",
    "dataset = create_dataset(config)\n",
    "logger.info(dataset)\n",
    "\n",
    "# dataset splitting\n",
    "train_data, valid_data, test_data = data_preparation(config, dataset)\n",
    "\n",
    "# model loading and initialization\n",
    "init_seed(config['seed'], config['reproducibility'])\n",
    "model = get_model(config['model'])(config, train_data.dataset).to(config['device'])\n",
    "logger.info(model)\n",
    "\n",
    "# # trainer loading and initialization\n",
    "# trainer = get_trainer(config['MODEL_TYPE'], config['model'])(config, model)\n",
    "\n",
    "# # model training\n",
    "# best_valid_score, best_valid_result = trainer.fit(\n",
    "#     train_data, valid_data, saved=saved, show_progress=config['show_progress']\n",
    "# )\n",
    "\n",
    "# # model evaluation\n",
    "# test_result = trainer.evaluate(test_data, load_best_model=saved, show_progress=config['show_progress'])\n",
    "\n",
    "# logger.info(set_color('best valid ', 'yellow') + f': {best_valid_result}')\n",
    "# logger.info(set_color('test result', 'yellow') + f': {test_result}')\n",
    "\n",
    "# {\n",
    "#     'best_valid_score': best_valid_score,\n",
    "#     'valid_score_bigger': config['valid_metric_bigger'],\n",
    "#     'best_valid_result': best_valid_result,\n",
    "#     'test_result': test_result\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6317422",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01 Sep 12:35    ERROR  Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: chingfhen. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\tanch\\Documents\\NTU\\NTU Year 4\\FYP - GNN\\code\\wandb\\run-20220901_123547-2dm0yjhr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/chingfhen/recbole/runs/2dm0yjhr\" target=\"_blank\">serene-sound-6</a></strong> to <a href=\"https://wandb.ai/chingfhen/recbole\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01 Sep 12:35    INFO  epoch 0 training [time: 2.00s, train loss: 29.8650]\n",
      "01 Sep 12:35    INFO  epoch 1 training [time: 0.25s, train loss: 24.7536]\n",
      "01 Sep 12:35    INFO  epoch 2 training [time: 0.27s, train loss: 13.1792]\n",
      "01 Sep 12:35    INFO  epoch 3 training [time: 0.25s, train loss: 10.5344]\n",
      "01 Sep 12:35    INFO  epoch 4 training [time: 0.26s, train loss: 9.9089]\n",
      "01 Sep 12:35    INFO  epoch 5 training [time: 0.26s, train loss: 9.5483]\n",
      "01 Sep 12:35    INFO  epoch 6 training [time: 0.23s, train loss: 9.3307]\n",
      "01 Sep 12:35    INFO  epoch 7 training [time: 0.25s, train loss: 9.1117]\n",
      "01 Sep 12:35    INFO  epoch 8 training [time: 0.28s, train loss: 8.9806]\n",
      "01 Sep 12:35    INFO  epoch 9 training [time: 0.24s, train loss: 8.8817]\n",
      "01 Sep 12:35    INFO  epoch 9 evaluating [time: 0.67s, valid_score: 0.094000]\n",
      "01 Sep 12:35    INFO  valid result: \n",
      "recall@10 : 0.0527    recall@20 : 0.0871    ndcg@10 : 0.0876    ndcg@20 : 0.094\n",
      "01 Sep 12:35    INFO  Saving current: saved/SimpleX-Sep-01-2022_12-35-53.pth\n",
      "01 Sep 12:35    INFO  epoch 10 training [time: 0.26s, train loss: 8.8054]\n",
      "01 Sep 12:35    INFO  epoch 11 training [time: 0.24s, train loss: 8.7148]\n",
      "01 Sep 12:35    INFO  epoch 12 training [time: 0.23s, train loss: 8.6617]\n",
      "01 Sep 12:35    INFO  epoch 13 training [time: 0.23s, train loss: 8.5488]\n",
      "01 Sep 12:35    INFO  epoch 14 training [time: 0.25s, train loss: 8.5535]\n",
      "01 Sep 12:36    INFO  epoch 15 training [time: 0.28s, train loss: 8.4349]\n",
      "01 Sep 12:36    INFO  epoch 16 training [time: 0.28s, train loss: 8.4558]\n",
      "01 Sep 12:36    INFO  epoch 17 training [time: 0.26s, train loss: 8.3872]\n",
      "01 Sep 12:36    INFO  epoch 18 training [time: 0.25s, train loss: 8.3482]\n",
      "01 Sep 12:36    INFO  epoch 19 training [time: 0.25s, train loss: 8.2876]\n",
      "01 Sep 12:36    INFO  epoch 19 evaluating [time: 0.69s, valid_score: 0.128800]\n",
      "01 Sep 12:36    INFO  valid result: \n",
      "recall@10 : 0.07    recall@20 : 0.1272    ndcg@10 : 0.114    ndcg@20 : 0.1288\n",
      "01 Sep 12:36    INFO  Saving current: saved/SimpleX-Sep-01-2022_12-35-53.pth\n",
      "01 Sep 12:36    INFO  epoch 20 training [time: 0.25s, train loss: 8.2819]\n",
      "01 Sep 12:36    INFO  epoch 21 training [time: 0.25s, train loss: 8.2574]\n",
      "01 Sep 12:36    INFO  epoch 22 training [time: 0.26s, train loss: 8.1653]\n",
      "01 Sep 12:36    INFO  epoch 23 training [time: 0.22s, train loss: 8.1945]\n",
      "01 Sep 12:36    INFO  epoch 24 training [time: 0.23s, train loss: 8.1230]\n",
      "01 Sep 12:36    INFO  epoch 25 training [time: 0.24s, train loss: 8.1557]\n",
      "01 Sep 12:36    INFO  epoch 26 training [time: 0.24s, train loss: 8.1109]\n",
      "01 Sep 12:36    INFO  epoch 27 training [time: 0.23s, train loss: 8.0530]\n",
      "01 Sep 12:36    INFO  epoch 28 training [time: 0.23s, train loss: 8.0305]\n",
      "01 Sep 12:36    INFO  epoch 29 training [time: 0.24s, train loss: 8.0037]\n",
      "01 Sep 12:36    INFO  epoch 29 evaluating [time: 0.84s, valid_score: 0.137200]\n",
      "01 Sep 12:36    INFO  valid result: \n",
      "recall@10 : 0.0736    recall@20 : 0.131    ndcg@10 : 0.1257    ndcg@20 : 0.1372\n",
      "01 Sep 12:36    INFO  Saving current: saved/SimpleX-Sep-01-2022_12-35-53.pth\n",
      "01 Sep 12:36    INFO  epoch 30 training [time: 0.24s, train loss: 8.0213]\n",
      "01 Sep 12:36    INFO  epoch 31 training [time: 0.23s, train loss: 8.0412]\n",
      "01 Sep 12:36    INFO  epoch 32 training [time: 0.23s, train loss: 8.0140]\n",
      "01 Sep 12:36    INFO  epoch 33 training [time: 0.24s, train loss: 7.9878]\n",
      "01 Sep 12:36    INFO  epoch 34 training [time: 0.24s, train loss: 7.9334]\n",
      "01 Sep 12:36    INFO  epoch 35 training [time: 0.24s, train loss: 7.9388]\n",
      "01 Sep 12:36    INFO  epoch 36 training [time: 0.25s, train loss: 7.9462]\n",
      "01 Sep 12:36    INFO  epoch 37 training [time: 0.24s, train loss: 7.9176]\n",
      "01 Sep 12:36    INFO  epoch 38 training [time: 0.26s, train loss: 7.9131]\n",
      "01 Sep 12:36    INFO  epoch 39 training [time: 0.23s, train loss: 7.8989]\n",
      "01 Sep 12:36    INFO  epoch 39 evaluating [time: 0.66s, valid_score: 0.150000]\n",
      "01 Sep 12:36    INFO  valid result: \n",
      "recall@10 : 0.0842    recall@20 : 0.1448    ndcg@10 : 0.1368    ndcg@20 : 0.15\n",
      "01 Sep 12:36    INFO  Saving current: saved/SimpleX-Sep-01-2022_12-35-53.pth\n",
      "01 Sep 12:36    INFO  epoch 40 training [time: 0.24s, train loss: 7.9014]\n",
      "01 Sep 12:36    INFO  epoch 41 training [time: 0.23s, train loss: 7.8222]\n",
      "01 Sep 12:36    INFO  epoch 42 training [time: 0.24s, train loss: 7.8497]\n",
      "01 Sep 12:36    INFO  epoch 43 training [time: 0.25s, train loss: 7.8471]\n",
      "01 Sep 12:36    INFO  epoch 44 training [time: 0.23s, train loss: 7.8303]\n",
      "01 Sep 12:36    INFO  epoch 45 training [time: 0.24s, train loss: 7.8373]\n",
      "01 Sep 12:36    INFO  epoch 46 training [time: 0.24s, train loss: 7.8135]\n",
      "01 Sep 12:36    INFO  epoch 47 training [time: 0.24s, train loss: 7.8159]\n",
      "01 Sep 12:36    INFO  epoch 48 training [time: 0.23s, train loss: 7.8300]\n",
      "01 Sep 12:36    INFO  epoch 49 training [time: 0.23s, train loss: 7.7960]\n",
      "01 Sep 12:36    INFO  epoch 49 evaluating [time: 0.74s, valid_score: 0.168500]\n",
      "01 Sep 12:36    INFO  valid result: \n",
      "recall@10 : 0.0987    recall@20 : 0.1601    ndcg@10 : 0.1566    ndcg@20 : 0.1685\n",
      "01 Sep 12:36    INFO  Saving current: saved/SimpleX-Sep-01-2022_12-35-53.pth\n",
      "01 Sep 12:36    INFO  Loading model structure and parameters from saved/SimpleX-Sep-01-2022_12-35-53.pth\n",
      "01 Sep 12:36    INFO  best valid : OrderedDict([('recall@10', 0.0987), ('recall@20', 0.1601), ('ndcg@10', 0.1566), ('ndcg@20', 0.1685)])\n",
      "01 Sep 12:36    INFO  test result: OrderedDict([('recall@10', 0.1092), ('recall@20', 0.1741), ('ndcg@10', 0.1807), ('ndcg@20', 0.1912)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_valid_score': 0.1685,\n",
       " 'valid_score_bigger': True,\n",
       " 'best_valid_result': OrderedDict([('recall@10', 0.0987),\n",
       "              ('recall@20', 0.1601),\n",
       "              ('ndcg@10', 0.1566),\n",
       "              ('ndcg@20', 0.1685)]),\n",
       " 'test_result': OrderedDict([('recall@10', 0.1092),\n",
       "              ('recall@20', 0.1741),\n",
       "              ('ndcg@10', 0.1807),\n",
       "              ('ndcg@20', 0.1912)])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trainer loading and initialization\n",
    "trainer = get_trainer(config['MODEL_TYPE'], config['model'])(config, model)\n",
    "\n",
    "# model training\n",
    "best_valid_score, best_valid_result = trainer.fit(\n",
    "    train_data, valid_data, saved=saved, show_progress=config['show_progress']\n",
    ")\n",
    "\n",
    "# model evaluation\n",
    "test_result = trainer.evaluate(test_data, load_best_model=saved, show_progress=config['show_progress'])\n",
    "\n",
    "logger.info(set_color('best valid ', 'yellow') + f': {best_valid_result}')\n",
    "logger.info(set_color('test result', 'yellow') + f': {test_result}')\n",
    "\n",
    "{\n",
    "    'best_valid_score': best_valid_score,\n",
    "    'valid_score_bigger': config['valid_metric_bigger'],\n",
    "    'best_valid_result': best_valid_result,\n",
    "    'test_result': test_result\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
